{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "from umap import UMAP\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "load_dotenv()\n",
    "sns.color_palette('colorblind')\n",
    "plt.style.use('Solarize_Light2')\n",
    "\n",
    "# Setting default DPI, pulling it from dotenv if it exists, setting it on 100 if not\n",
    "\n",
    "try:\n",
    "    pc_dpi = int(os.getenv('DPI'))\n",
    "except TypeError:\n",
    "    pc_dpi = 100\n",
    "if pc_dpi is None:\n",
    "    pc_dpi = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP object creation :\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# nlp._config  # Checking : tok2vec is in config by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading cleaned dataset, using pickle allows type preservation (spacy doc, np array etc.)\n",
    "\n",
    "df = pd.read_pickle(filepath_or_buffer=\"../data/ecommerce_cleaned.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 : Text classification using sparse representation (Bag of Words & TF-IDF)\n",
    "\n",
    "- We will first try to classify the products using the sparse representation of their descriptions. We will use Bag of Words (abbreviated as BoW or CV) and TF-IDF to represent the product descriptions. We will try to predict the categories using a Multinomial Naive Bayes model.\n",
    "- As this approach will generate sparse vectors (vectors containing mostly zeros), we will try to apply a dimension reduction technique, UMAP, to reduce the size of the vectors to 2 components and attempt a classification on these components.\n",
    "- We will evaluate the models using the average accuracy of the predictions, sklearn's classification report, and, if necessary, a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Note : Since we applied tokenization, stemming and lemmentazation in notebook `nb_01` and we preserved the types by using a pickle format, it is not necessary to clean the text in this notebook.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Feature exctraction :\n",
    "&emsp;We will apply CountVectorizer & TfidfVectorizer on the whole corpus first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords have already been cleaned so we dont need to add them to the vectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(row):\n",
    "    \"\"\"\n",
    "    returns the vector of the spacy.doc object in col doc_desc\n",
    "    \"\"\"\n",
    "    desc_doc = row[\"doc_desc\"]\n",
    "    desc_vec = desc_doc.vector\n",
    "    return desc_vec\n",
    "\n",
    "\n",
    "def list_to_str(row):\n",
    "    \"\"\"\n",
    "    Turns lem_desc, a list of tokens, into a string for CV and TF-IDF usage,\n",
    "    returns string\n",
    "    \"\"\"\n",
    "    desc_list = row[\"lem_desc\"]\n",
    "    desc_txt = \" \".join(desc_list)\n",
    "    return desc_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_vec\"] = df.apply(func=get_vector, axis=1)\n",
    "df[\"lem_desc_txt\"] = df.apply(func=list_to_str, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():  # Storing the results of the transformation in variables for UMAP\n",
    "    warnings.simplefilter('ignore')\n",
    "    count_vectorizer_transform = count_vectorizer.fit_transform(df[\"lem_desc_txt\"])\n",
    "    tf_idf_transform = tf_idf_vectorizer.fit_transform(df[\"lem_desc_txt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both should be of shape (len(df), n)\n",
    "\n",
    "print(count_vectorizer_transform.shape)\n",
    "print(tf_idf_transform.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.__len__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Multinomial Naive Bayes so we need to encode the categories as integers and not txt:\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"enc_category\"] = le.fit_transform(df[\"first_category\"])\n",
    "df[[\"first_category\", \"enc_category\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"lem_desc_txt\"],\n",
    "    df[\"enc_category\"],\n",
    "    test_size=0.3,\n",
    "    random_state=123\n",
    "    )\n",
    "\n",
    "# The split will be common to bow & tf-idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 : Predictions using Bag of words and tf_idf :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 : Bag of words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying count vectorizer previously fitted on the corpus on the data :\n",
    "X_train_bow = count_vectorizer.transform(X_train)\n",
    "X_test_bow = count_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and fitting model :\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X=X_train_bow, y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bow = clf.predict(X_test_bow)\n",
    "\n",
    "accuracy_bow = accuracy_score(y_true=y_test, y_pred=y_pred_bow)\n",
    "\n",
    "print(accuracy_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_bow_pred = le.inverse_transform(y_pred_bow)\n",
    "inv_true = le.inverse_transform(y_test)\n",
    "\n",
    "# This reverses the encoding on labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=inv_true, y_pred=inv_bow_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations :\n",
    "\n",
    "- The model achieves a very high average precision of 90/91%.\n",
    "- It is important to note that, due to the limited amount of data (see support), the model might benefit training on a larger set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_bow = pd.DataFrame(confusion_matrix(y_true=inv_true, y_pred=inv_bow_pred))\n",
    "conf_matrix_bow.columns = le.inverse_transform(conf_matrix_bow.columns)\n",
    "conf_matrix_bow.index = le.inverse_transform(conf_matrix_bow.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(4, 3),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "sns.heatmap(conf_matrix_bow, annot=True, cmap=\"YlGnBu\", xticklabels=True, yticklabels=True)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "plt.tick_params(labelsize=3)\n",
    "fig.suptitle(\"Confusion matrix : classification on bag of words approach\")\n",
    "#\n",
    "###\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix :\n",
    "\n",
    "- This matrix shows in details what could be observed in the classification report : the model's overall performances are good but might benefit from more data.\n",
    "- It's also interesting to note that there were 5 items in the \"multimedia\" class that were misclassified as \"watches\". It is logical as these two categories share a lot of technical terms. This is might be the source of the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 : TF-IDF \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tf_idf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = tf_idf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X=X_train_tfidf, y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf = clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_tfidf = accuracy_score(y_true=y_test, y_pred=y_pred_tfidf)\n",
    "\n",
    "print(accuracy_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_tfidf_pred = le.inverse_transform(y_pred_tfidf)\n",
    "inv_true = le.inverse_transform(y_test)\n",
    "\n",
    "# This reverses the encoding on labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=inv_true, y_pred=inv_tfidf_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_tfidf = pd.DataFrame(confusion_matrix(y_true=inv_true, y_pred=inv_tfidf_pred))\n",
    "conf_matrix_tfidf.columns = le.inverse_transform(conf_matrix_tfidf.columns)\n",
    "conf_matrix_tfidf.index = le.inverse_transform(conf_matrix_tfidf.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(4, 3),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "sns.heatmap(conf_matrix_tfidf, annot=True, cmap=\"YlGnBu\", xticklabels=True, yticklabels=True)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "plt.tick_params(labelsize=3)\n",
    "fig.suptitle(\"Confusion matrix : classification with tf-idf approach\")\n",
    "#\n",
    "###\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations :\n",
    "- This model performs quite well overall, with better performance in some categories than BoW but not as well in others (e.g. baby care and home furnishing).\n",
    "- We could potentially improve the model by providing it with a larger training set and using hyperparameter tuning techniques like GridSearchCV to adjust the alpha of the Multinomial Naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 : UMAP on vectors from count_vectorizer and tf-idf :\n",
    "\n",
    "- This allows to reduce the dimensions to 2838 to 2 (It is worth noting that these vectors are expected to grow with a larger dataset)\n",
    "- UMAP is globally faster than T-SNE for often better results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 : UMAP reduction\n",
    "- Using UMAP to reduce the data into two components for each approach\n",
    "- Using MinMax scaler on the data to be able to use Naive Bayes (which does not work on negative values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df.copy()  # Separating the two datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction using UMAP, settings by default, 2 components\n",
    "\n",
    "umap = UMAP(n_components=2, n_jobs=-1)\n",
    "\n",
    "umap_cv = umap.fit_transform(count_vectorizer_transform)\n",
    "umap_tfidf = umap.fit_transform(tf_idf_transform)\n",
    "\n",
    "print(umap_cv.shape)\n",
    "print(umap_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving component 0 and 1 for CountVectorizer (umap_cv) and tf-idf (umap_tfidf)\n",
    "# assigning 2 cols for each method\n",
    "\n",
    "df_reduced[\"umap_cv_comp_0\"] = np.nan\n",
    "df_reduced[\"umap_cv_comp_1\"] = np.nan\n",
    "df_reduced[\"umap_tfidf_comp_0\"] = np.nan\n",
    "df_reduced[\"umap_tfidf_comp_1\"] = np.nan\n",
    "\n",
    "for index in range(0, len(df)):\n",
    "    df_reduced.loc[index, \"umap_cv_comp_0\"] = umap_cv[index][0]\n",
    "    df_reduced.loc[index, \"umap_cv_comp_1\"] = umap_cv[index][1]\n",
    "    df_reduced.loc[index, \"umap_tfidf_comp_0\"] = umap_tfidf[index][0]\n",
    "    df_reduced.loc[index, \"umap_tfidf_comp_1\"] = umap_tfidf[index][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmx = MinMaxScaler()  # Avoiding values < 0\n",
    "\n",
    "df_reduced[\"umap_cv_comp_0\"] = mmx.fit_transform(df_reduced[\"umap_cv_comp_0\"].values.reshape(-1, 1))\n",
    "df_reduced[\"umap_cv_comp_1\"] = mmx.fit_transform(df_reduced[\"umap_cv_comp_1\"].values.reshape(-1, 1))\n",
    "df_reduced[\"umap_tfidf_comp_0\"] = mmx.fit_transform(df_reduced[\"umap_tfidf_comp_0\"].values.reshape(-1, 1))\n",
    "df_reduced[\"umap_tfidf_comp_1\"] = mmx.fit_transform(df_reduced[\"umap_tfidf_comp_1\"].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original text next to reduced components for cv and tf-idf :\n",
    "\n",
    "display_cols = [\n",
    "    \"lem_desc_txt\", \"umap_cv_comp_0\", \"umap_cv_comp_1\",\n",
    "    \"umap_tfidf_comp_0\", \"umap_tfidf_comp_1\"\n",
    "    ]\n",
    "\n",
    "display(df_reduced[display_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisations :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BoW :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(4, 4),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "g = sns.scatterplot(data=df_reduced, x=\"umap_cv_comp_0\", y=\"umap_cv_comp_1\", hue=\"first_category\", ax=ax1)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "sns.move_legend(\n",
    "    ax1, \"upper right\",\n",
    "    ncol=2,\n",
    "    title=None,\n",
    "    frameon=True,\n",
    ")\n",
    "plt.setp(ax1.get_legend().get_texts(), fontsize=\"4\")\n",
    "ax1.legend(bbox_to_anchor= (1,1))\n",
    "fig.suptitle(\"UMAP reduced count vectorizer with first categories of products\")\n",
    "#\n",
    "###\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(5, 5),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "g = sns.scatterplot(data=df_reduced, x=\"umap_tfidf_comp_0\", y=\"umap_tfidf_comp_1\", hue=\"first_category\", ax=ax1)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "sns.move_legend(\n",
    "    ax1, \"upper right\",\n",
    "    ncol=2,\n",
    "    title=None,\n",
    "    frameon=True,\n",
    ")\n",
    "plt.setp(ax1.get_legend().get_texts(), fontsize=\"5\")\n",
    "ax1.legend(bbox_to_anchor=(1, 1))\n",
    "fig.suptitle(\"UMAP reduced TF-IDF with first categories of products\")\n",
    "#\n",
    "###\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation :\n",
    "\n",
    "- On both methods, the categories do not seem to exhibit major differences\n",
    "- The loss of information might be too important for this approach to be relevant\n",
    "- Reduction on tf-idf might be a little bit more pertinent as we see some clusters like baby care and watches isolated on the scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 : Classification on Bag of Words, UMAP reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_bow = [\"umap_cv_comp_0\", \"umap_cv_comp_1\", \"enc_category\"]\n",
    "\n",
    "df_bow_umap = df_reduced[cols_bow]  # Creating a df for bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow_umap, X_test_bow_umap, y_train_bow_umap, y_test_bow_umap = train_test_split(\n",
    "    df_bow_umap[[\"umap_cv_comp_0\", \"umap_cv_comp_1\"]],\n",
    "    df_bow_umap[\"enc_category\"],\n",
    "    test_size=0.3,\n",
    "    random_state=123  # Keeping the same seed\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train_bow_umap, y_train_bow_umap)\n",
    "\n",
    "predictions = clf.predict(X_test_bow_umap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_bow_umap = accuracy_score(y_true=y_test_bow_umap, y_pred=predictions)\n",
    "\n",
    "print(accuracy_bow_umap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok that seems bad, let's try a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_bow_umap, predictions)\n",
    "\n",
    "confusion_matrix_df_bow_umap = pd.DataFrame(data=conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(confusion_matrix_df_bow_umap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation :\n",
    "\n",
    "- Overall, the loss of information during UMAP reduction seems to be too large for the model to be precise, with an accuracy of only 24.4%. It is likely that much of the variance was lost during the dimension reduction process, making it difficult for the classification model to be relevant on BoW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 : Classification on TF-IDF, UMAP reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_tf_idf = [\"umap_tfidf_comp_0\", \"umap_tfidf_comp_1\", \"enc_category\"]\n",
    "\n",
    "df_tf_idf_umap = df_reduced[cols_tf_idf]  # Creating a df for tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_umap, X_test_tfidf_umap, y_train_tfidf_umap, y_test_tfidf_umap = train_test_split(\n",
    "    df_tf_idf_umap[[\"umap_tfidf_comp_0\", \"umap_tfidf_comp_1\"]],\n",
    "    df_tf_idf_umap[\"enc_category\"],\n",
    "    test_size=0.3,\n",
    "    random_state=123  # same seed\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train_tfidf_umap, y_train_tfidf_umap)\n",
    "\n",
    "predictions = clf.predict(X_test_tfidf_umap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tfidf_umap = accuracy_score(y_true=y_test_tfidf_umap, y_pred=predictions)\n",
    "\n",
    "print(accuracy_tfidf_umap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test_tfidf_umap, predictions)\n",
    "\n",
    "confusion_matrix_df_umap = pd.DataFrame(data=conf_matrix)\n",
    "\n",
    "display(confusion_matrix_df_umap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation :\n",
    "\n",
    "- Based on the confusion matrix and the reported accuracy of 14.4%, it looks like the model is not performing well. Most of the predictions made by the model are incorrect, with a high number of false positives and false negatives.\n",
    "- The dimensional reduction might be to blame for the poor performance of the model. We should consider discarding the dimensional reduction approach in order to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_bow_umap\n",
    "del df_tf_idf_umap\n",
    "del accuracy_bow_umap\n",
    "del accuracy_tfidf_umap\n",
    "# Frees a bit of memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 : Conclusion on BoW and TF-IDF\n",
    "\n",
    "- Dimensional reduction is not efficient in this case, the loss of information seems to be too large.\n",
    "- Both unreduced models, however, seems to perform quite well. As explained in both cases, we could benefit from a larger sample of data but, from the informations we have, both models could be used, bag of words seems to be more accurate overall but it might depend on the training.\n",
    "- It is important to remember that the number of descriptions directly affects the length of the vectors generated by these approaches. As the number of descriptions increases, it is highly likely that the size of the matrices (which are linked to the size of the corpus' vocabulary) will grow, potentially negatively impacting the model's performance in terms of speed and memory usage.\n",
    "<br><br><hr><br>\n",
    "- Using word and sentence embedding might lead to better performances and better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 : Word embedding :\n",
    "\n",
    "&emsp;In the context of text classification, word embedding can be a pretty potent method over BoW and TF-IDF methods :\n",
    "- Word embedding captures the semantic meaning of a word (if the model is well-trained), which, in the context of product classification, might present some advantages.\n",
    "- It also presents the advantage of being a \"dense\" method, compared to BoW and TF-IDF which are \"sparse\" methods. Our corpus is quite small, but we can see that the BoW and TF-IDF vectors are very long (2838), but still manageable. If we want to upscale these methods, it would mean that these vectors would be as long as there are unique terms in the dataset, which would present a big computational and size issue. As word embedding represents a word in a vector of size (usually) 300, upscaling wouldn't be such of a problem.\n",
    "\n",
    "<i>In a context where the dataset would be a much bigger sample, we could create our own word embedding model. However, since we have only 1048 product descriptions, creating a model based on this data might not be relevant enough, so we will use a pretrained model instead. It might be more accurate with more data or with a training dataset using only e-commerce data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 : Selection of the model\n",
    "\n",
    "&emsp;We have a lot of options (Google's word2vec on Google News, Stanford's GloVe on Wikipedia and Meta's Fasttext). These models are quite heavy (around 1Gb) but contain a lot of informations. Google's model is the largest but Fasttest can also be interesting to use as it is trained on Wikipedia's corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "f56a869c4daa215a59dfefe9df0caca71d11de3c8dc85895ad84467caf29dffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
